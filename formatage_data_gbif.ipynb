{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab2d9fc-467a-4e9f-a460-b698c3501615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import CRS\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "import importlib\n",
    "import fonctions_annexes_biodiv\n",
    "importlib.reload(fonctions_annexes_biodiv)\n",
    "from fonctions_annexes_biodiv import generer_dictionnaire_taxonomie\n",
    "\n",
    "%matplotlib qt\n",
    "path = 'C:/Users/anormand/Documents/Projet Python/Biodiv/Data'  # chemin vers le fichier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3786d35-69d5-450d-aaab-4239250c3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrees_per_km(latitude):\n",
    "    \"\"\"\n",
    "    Calculate the degree equivalent of 1 km for both latitude and longitude\n",
    "    at a specific latitude.\n",
    "\n",
    "    :param latitude: Latitude in degrees\n",
    "    :return: Tuple (lat_deg_per_km, lon_deg_per_km)\n",
    "    \"\"\"\n",
    "    # 1 degree of latitude is ~111.32 km everywhere\n",
    "    lat_deg_per_km = 1 / 111.32\n",
    "\n",
    "    # 1 degree of longitude varies with latitude\n",
    "    lon_deg_per_km = 1 / 111.32 / math.cos(math.radians(latitude))\n",
    "\n",
    "\n",
    "    return lat_deg_per_km, lon_deg_per_km\n",
    "\n",
    "def create_country_grid_WGS84(gdf, country_code,col_code=\"color_code\", grid_size_km=10,midpoint_lat=None,display=False):\n",
    "    \"\"\"\n",
    "    Generates a grid within the specified country's boundaries with cell names based on \n",
    "    coordinates and grid size using WGS 84 reference system (EPSG:4326).\n",
    "    \n",
    "    Parameters:\n",
    "    - gdf: GeoDataFrame containing global boundaries with a column for the country's unique code (e.g., \"color_code\").\n",
    "    - country_name: Name of the country for which to generate the grid.\n",
    "    - grid_size_km: Size of the grid cells in kilometers (default is 10 km x 10 km).\n",
    "    \n",
    "    Returns:\n",
    "    - grid: GeoDataFrame with grid cells and a 'name' column for each cell's unique identifier.\n",
    "    \"\"\"\n",
    "    # Convert grid size from kilometers to degrees (approximation for WGS 84)\n",
    "    # 1 degree of latitude ≈ 111 km, but longitude degrees vary by latitude\n",
    "    # Filter to get the specified country\n",
    "    country= gdf[gdf[col_code] == country_code]\n",
    "\n",
    "    if country.empty:\n",
    "        raise ValueError(f\"Country '{country_code}' not found in the GeoDataFrame.\")\n",
    "\n",
    "    # Define a function to create a grid of specified resolution (in degrees) with alignment to a reference point\n",
    "    def create_grid(country, grid_size=10, reference_point=(0, 0),midpoint_lat=None):\n",
    "        # Unpack reference point\n",
    "        ref_x, ref_y = reference_point\n",
    "        \n",
    "        country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
    "    \n",
    "        # Get the bounds of the input GeoDataFrame\n",
    "        minx, miny, maxx, maxy = country_geometry.bounds\n",
    "\n",
    "\n",
    "            # Step 3: Adjust bounding box to align with (0, 0)\n",
    "        # Use approximate degrees per km for the middle of the country\n",
    "        if midpoint_lat is None:\n",
    "            midpoint_lat = (miny + maxy) / 2\n",
    "        lat_deg_per_km, lon_deg_per_km = degrees_per_km(midpoint_lat)\n",
    "        dy = grid_size * lat_deg_per_km\n",
    "        dx = grid_size * lon_deg_per_km\n",
    "    \n",
    "        # Align the starting point to the reference point\n",
    "        start_x = ref_x + ((minx - ref_x) // dx) * dx\n",
    "        start_y = ref_y + ((miny - ref_y) // dy) * dy\n",
    "    \n",
    "        grid_cells = []\n",
    "        x = start_x\n",
    "        while x < maxx:\n",
    "            y = start_y\n",
    "            while y < maxy:\n",
    "                cell = box(x, y, x + dx, y + dy)\n",
    "    \n",
    "                min_lon, min_lat, max_lon, max_lat = cell.bounds\n",
    "    \n",
    "                # Append the grid cell along with its bounds\n",
    "                grid_cells.append({\n",
    "                    \"geometry\": cell,\n",
    "                    \"min_lon\": min_lon,\n",
    "                    \"min_lat\": min_lat,\n",
    "                    \"max_lon\": max_lon,\n",
    "                    \"max_lat\": max_lat\n",
    "                })\n",
    "                y += dy\n",
    "            x += dx\n",
    "    \n",
    "        # Create a GeoDataFrame from the grid cells with the additional columns\n",
    "        grid_gdf = gpd.GeoDataFrame(\n",
    "            grid_cells, \n",
    "            columns=[\"geometry\", \"min_lon\", \"min_lat\", \"max_lon\", \"max_lat\"], \n",
    "            crs=country.crs\n",
    "        )\n",
    "        \n",
    "        return grid_gdf\n",
    "\n",
    "    \n",
    "    # Create the grid and clip it to the country's boundary\n",
    "\n",
    "    grid = create_grid(country, grid_size=grid_size_km,midpoint_lat=midpoint_lat)\n",
    "    grid = grid[grid.intersects(country.unary_union)]\n",
    "    \n",
    "    # Generate names based on cell centroid latitude and longitude\n",
    "    def generate_grid_name(cell, grid_size_km):\n",
    "        centroid = cell.geometry.centroid\n",
    "        lon, lat = centroid.x, centroid.y\n",
    "        lon_label = f\"E{int(abs(lon) * 100):03d}\" if lon >= 0 else f\"W{int(abs(lon) * 100):03d}\"\n",
    "        lat_label = f\"N{int(abs(lat) * 100):03d}\" if lat >= 0 else f\"S{int(abs(lat) * 100):03d}\"\n",
    "        return f\"{grid_size_km}km{lon_label}{lat_label}\"\n",
    "\n",
    "    # Apply naming to each grid cell\n",
    "    grid[\"cell_name\"] = grid.apply(lambda cell: generate_grid_name(cell, grid_size_km), axis=1)\n",
    "    grid[\"country_code\"] = country_code\n",
    "\n",
    "    if display is True:\n",
    "        # Plot the grid and the country boundary for reference\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        gdf.plot(ax=ax, edgecolor=\"black\", linewidth=0.5)\n",
    "        country.plot(ax=ax, edgecolor=\"red\", linewidth=2, facecolor=\"none\")\n",
    "        grid.plot(ax=ax, color=\"lightblue\", edgecolor=\"grey\", alpha=0.6)\n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def add_grid_to_country(df_country, grid,cle_geo):\n",
    "    \"\"\"\n",
    "    Optimized version of adding the corresponding grid cell to each row in df_country based on latitude and longitude.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_country: DataFrame containing the columns 'decimalLatitude' and 'decimalLongitude'.\n",
    "    - grid: DataFrame containing the grid cells with 'name', 'min_lon', 'min_lat', 'max_lon', and 'max_lat' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - df_country: Updated DataFrame with an additional 'grid_name' column indicating the grid cell for each point.\n",
    "    \"\"\"\n",
    "    # Convert grid bounds to NumPy arrays for efficient vectorized comparison\n",
    "    min_lons = grid['min_lon'].values\n",
    "    max_lons = grid['max_lon'].values\n",
    "    min_lats = grid['min_lat'].values\n",
    "    max_lats = grid['max_lat'].values\n",
    "    grid_names = grid[cle_geo].values\n",
    "    \n",
    "    # Initialize an array to store the grid names\n",
    "    grid_names_for_points = []\n",
    "    \n",
    "    # Iterate over each point in df_country and apply vectorized comparison\n",
    "    for lon, lat in zip(df_country['decimalLongitude'], df_country['decimalLatitude']):\n",
    "        # Find the grid cell by comparing the point coordinates with grid bounds\n",
    "        matching_grid = np.where((min_lons <= lon) & (lon <= max_lons) & (min_lats <= lat) & (lat <= max_lats))[0]\n",
    "        \n",
    "        if matching_grid.size > 0:\n",
    "            grid_names_for_points.append(grid_names[matching_grid[0]])  # Take the first matching grid cell\n",
    "        else:\n",
    "            grid_names_for_points.append(None)  # No matching grid\n",
    "    \n",
    "    # Add the grid names to the DataFrame\n",
    "    df_country['grid_name'] = grid_names_for_points\n",
    "    \n",
    "    return df_country\n",
    "\n",
    "def formater_maille_espece_GBIF(df,cle_geo='codeMaille10Km',cle_ID='cdRef',annee_min=None,bornes_temporelles=None):\n",
    "    df_dico=generer_dictionnaire_taxonomie(df,cle_ID)\n",
    "    # Convertir la colonne 'year' en int\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Convertir la colonne 'cdNom' en int\n",
    "    df[cle_ID] = df[cle_ID].astype(int)\n",
    "\n",
    "    if annee_min is not None:\n",
    "        df=df[df['year']>=annee_min]\n",
    "    \n",
    "    # Choisir des bornes temporelles et assigner une période aux données\n",
    "    if bornes_temporelles is not None:\n",
    "        df.loc[:, 'periode'] = pd.cut(df['year'], bins=bornes_temporelles, \n",
    "                       labels=[f'Période {i+1}: {bornes_temporelles[i]+1} à {bornes_temporelles[i+1]}' for i in range(len(bornes_temporelles) - 1)],\n",
    "                       include_lowest=False)  # include_lowest=True inclut la borne inférieureure\n",
    "        # Compter le nombre de données dans chaque intervalle\n",
    "        compte_par_periode = df['periode'].value_counts()\n",
    "        print(compte_par_periode)\n",
    "         # Compter les occurrences d'observation de chaque taxon pour chaque code et période\n",
    "        df_maille_espece = df.groupby([cle_geo, cle_ID,'periode'], observed=True).size().reset_index(name='nombreObs')\n",
    "        #df_maille_espece = df.groupby([cle_geo, cle_ID,'periode'], observed=True)['individualCount'].sum().reset_index(name='nombreObs')\n",
    "        #eventuellement remplacer ['individualCount'].sum() par .size() \n",
    "       \n",
    "    else:\n",
    "        # Compter les occurrences d'observation de chaque taxon pour chaque code\n",
    "        df_maille_espece = df.groupby([cle_geo, cle_ID], observed=True).size().reset_index(name='nombreObs')\n",
    "        #eventuellement remplacer ['individualCount'].sum() par .size() \n",
    "        \n",
    "    df_maille_espece=pd.merge(df_maille_espece,df_dico,on=cle_ID)\n",
    "    \n",
    "    return df_maille_espece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c26f6d-8e07-4300-a87f-0368e4dfca84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0822afb7-ab1f-4b78-b8e5-4b912c956901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the world boundaries\n",
    "\n",
    "world_terrestre = gpd.read_file(path+\"/SIG_global/world-administrative-boundaries.geojson\")\n",
    "world_maritime = gpd.read_file(path+\"/SIG_global/eez_v11.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fe4b3-ddcc-4d31-b946-6a4e7f821922",
   "metadata": {},
   "outputs": [],
   "source": [
    "departement_fichier = r'C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_France\\SIG\\carte_departements.geojson'  # Remplace par le chemin vers ton fichier\n",
    "departement_gpd = gpd.read_file(departement_fichier)\n",
    "\n",
    "PNR_fichier = r'C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_France\\SIG\\N_ENP_PNR_S_000.shx'  # Remplace par le chemin vers ton fichier\n",
    "PNR_gpd = gpd.read_file(PNR_fichier)\n",
    "PNR_gpd=PNR_gpd[['NOM_SITE','geometry']]\n",
    "\n",
    "PN_fichier = r'C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_France\\SIG\\N_ENP_PN_S_000.shx'  # Remplace par le chemin vers ton fichier\n",
    "PN_gpd = gpd.read_file(PN_fichier)\n",
    "PN_gpd=PN_gpd[['NOM_SITE','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee03cf7-bdca-434a-9325-808c64f9ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "401cb77a-0a2e-4bb9-a4ac-244929410247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON file\n",
    "world_terrestre = gpd.read_file(path + \"/SIG_global/world-administrative-boundaries.geojson\")\n",
    "\n",
    "# Update the name from \"United Republic of Tanzania\" to \"Tanzania\"\n",
    "world_terrestre.loc[world_terrestre['color_code'] == \"SRB\", 'color_code'] = \"KV\"\n",
    "\n",
    "# Save the updated GeoDataFrame back to a GeoJSON file if needed\n",
    "world_terrestre.to_file(path + \"/SIG_global/world-administrative-boundaries.geojson\", driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4de64ced-c008-4990-9a31-19fc287eee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\raw.py:698: UserWarning: 'crs' was not provided.  The output dataset will not have projection information defined and may not be usable in other systems.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "world_terrestre.to_file(path + \"/SIG_global/world-administrative-boundaries.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ee317fb-8ca5-489e-97dd-f41f3dafe04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geo_point_2d</th>\n",
       "      <th>iso3</th>\n",
       "      <th>status</th>\n",
       "      <th>color_code</th>\n",
       "      <th>name</th>\n",
       "      <th>continent</th>\n",
       "      <th>region</th>\n",
       "      <th>iso_3166_1_alpha_2_codes</th>\n",
       "      <th>french_short</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>{ \"lon\": 20.805271723235375, \"lat\": 44.0314984...</td>\n",
       "      <td>SRB</td>\n",
       "      <td>Member State</td>\n",
       "      <td>SRB</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "      <td>RS</td>\n",
       "      <td>Serbie</td>\n",
       "      <td>POLYGON ((20.26102 46.11485, 20.31403 46.06986...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          geo_point_2d iso3        status  \\\n",
       "256  { \"lon\": 20.805271723235375, \"lat\": 44.0314984...  SRB  Member State   \n",
       "\n",
       "    color_code    name continent           region iso_3166_1_alpha_2_codes  \\\n",
       "256        SRB  Serbia    Europe  Southern Europe                       RS   \n",
       "\n",
       "    french_short                                           geometry  \n",
       "256       Serbie  POLYGON ((20.26102 46.11485, 20.31403 46.06986...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_terrestre[world_terrestre['name'].str.contains(\"Serbia\",case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8bd7377-86d4-4eeb-860b-d90976b2a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = world_terrestre.loc[3].copy()\n",
    "new_row[\"color_code\"] = \"SRB\"  # Remplace par la valeur souhaitée\n",
    "new_row[\"name\"] = \"Serbia\"  # Remplace par la valeur souhaitée\n",
    "\n",
    "# Ajouter la ligne dupliquée au DataFrame\n",
    "world_terrestre = pd.concat([world_terrestre, new_row.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e5919c-f0e2-4e1d-92fb-229d61a1efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du pays\n",
    "\n",
    "country_name=\"Albania\"\n",
    "grid_size_km=20\n",
    "cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "country_code = world_terrestre[world_terrestre[\"name\"] == country_name][\"color_code\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8971151a-5725-493c-b2f0-ee806255bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons trouvés :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\362260744.py:3: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>min_lon</th>\n",
       "      <th>min_lat</th>\n",
       "      <th>max_lon</th>\n",
       "      <th>max_lat</th>\n",
       "      <th>codeMaille20Km</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLYGON ((19.32727 40.24434, 19.32727 40.424, ...</td>\n",
       "      <td>19.088666</td>\n",
       "      <td>40.244341</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.424003</td>\n",
       "      <td>20kmE1920N4033</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>POLYGON ((19.32727 40.424, 19.32727 40.60367, ...</td>\n",
       "      <td>19.088666</td>\n",
       "      <td>40.424003</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.603665</td>\n",
       "      <td>20kmE1920N4051</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>POLYGON ((19.32727 40.60367, 19.32727 40.78333...</td>\n",
       "      <td>19.088666</td>\n",
       "      <td>40.603665</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.783327</td>\n",
       "      <td>20kmE1920N4069</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>POLYGON ((19.56588 40.06468, 19.56588 40.24434...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.064678</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.244341</td>\n",
       "      <td>20kmE1944N4015</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>POLYGON ((19.56588 40.24434, 19.56588 40.424, ...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.244341</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.424003</td>\n",
       "      <td>20kmE1944N4033</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>POLYGON ((19.56588 40.424, 19.56588 40.60367, ...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.424003</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.603665</td>\n",
       "      <td>20kmE1944N4051</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>POLYGON ((19.56588 40.60367, 19.56588 40.78333...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.603665</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.783327</td>\n",
       "      <td>20kmE1944N4069</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>POLYGON ((19.56588 40.78333, 19.56588 40.96299...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.783327</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.962990</td>\n",
       "      <td>20kmE1944N4087</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>POLYGON ((19.56588 40.96299, 19.56588 41.14265...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>40.962990</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.142652</td>\n",
       "      <td>20kmE1944N4105</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>POLYGON ((19.56588 41.14265, 19.56588 41.32231...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>41.142652</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.322314</td>\n",
       "      <td>20kmE1944N4123</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>POLYGON ((19.56588 41.32231, 19.56588 41.50198...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>41.322314</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.501976</td>\n",
       "      <td>20kmE1944N4141</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>POLYGON ((19.56588 41.50198, 19.56588 41.68164...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>41.501976</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.681639</td>\n",
       "      <td>20kmE1944N4159</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>POLYGON ((19.56588 41.68164, 19.56588 41.8613,...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>41.681639</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.861301</td>\n",
       "      <td>20kmE1944N4177</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>POLYGON ((19.56588 41.8613, 19.56588 42.04096,...</td>\n",
       "      <td>19.327275</td>\n",
       "      <td>41.861301</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>42.040963</td>\n",
       "      <td>20kmE1944N4195</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>POLYGON ((19.80449 39.88502, 19.80449 40.06468...</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>39.885016</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>40.064678</td>\n",
       "      <td>20kmE1968N3997</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>POLYGON ((19.80449 40.06468, 19.80449 40.24434...</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>40.064678</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>40.244341</td>\n",
       "      <td>20kmE1968N4015</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>POLYGON ((19.80449 41.50198, 19.80449 41.68164...</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.501976</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>41.681639</td>\n",
       "      <td>20kmE1968N4159</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>POLYGON ((19.80449 41.68164, 19.80449 41.8613,...</td>\n",
       "      <td>19.565883</td>\n",
       "      <td>41.681639</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>41.861301</td>\n",
       "      <td>20kmE1968N4177</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>POLYGON ((20.0431 39.52569, 20.0431 39.70535, ...</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>39.525692</td>\n",
       "      <td>20.043100</td>\n",
       "      <td>39.705354</td>\n",
       "      <td>20kmE1992N3961</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>POLYGON ((20.0431 39.70535, 20.0431 39.88502, ...</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>39.705354</td>\n",
       "      <td>20.043100</td>\n",
       "      <td>39.885016</td>\n",
       "      <td>20kmE1992N3979</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>POLYGON ((20.0431 39.88502, 20.0431 40.06468, ...</td>\n",
       "      <td>19.804491</td>\n",
       "      <td>39.885016</td>\n",
       "      <td>20.043100</td>\n",
       "      <td>40.064678</td>\n",
       "      <td>20kmE1992N3997</td>\n",
       "      <td>ALB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             geometry    min_lon    min_lat  \\\n",
       "4   POLYGON ((19.32727 40.24434, 19.32727 40.424, ...  19.088666  40.244341   \n",
       "5   POLYGON ((19.32727 40.424, 19.32727 40.60367, ...  19.088666  40.424003   \n",
       "6   POLYGON ((19.32727 40.60367, 19.32727 40.78333...  19.088666  40.603665   \n",
       "21  POLYGON ((19.56588 40.06468, 19.56588 40.24434...  19.327275  40.064678   \n",
       "22  POLYGON ((19.56588 40.24434, 19.56588 40.424, ...  19.327275  40.244341   \n",
       "23  POLYGON ((19.56588 40.424, 19.56588 40.60367, ...  19.327275  40.424003   \n",
       "24  POLYGON ((19.56588 40.60367, 19.56588 40.78333...  19.327275  40.603665   \n",
       "25  POLYGON ((19.56588 40.78333, 19.56588 40.96299...  19.327275  40.783327   \n",
       "26  POLYGON ((19.56588 40.96299, 19.56588 41.14265...  19.327275  40.962990   \n",
       "27  POLYGON ((19.56588 41.14265, 19.56588 41.32231...  19.327275  41.142652   \n",
       "28  POLYGON ((19.56588 41.32231, 19.56588 41.50198...  19.327275  41.322314   \n",
       "29  POLYGON ((19.56588 41.50198, 19.56588 41.68164...  19.327275  41.501976   \n",
       "30  POLYGON ((19.56588 41.68164, 19.56588 41.8613,...  19.327275  41.681639   \n",
       "31  POLYGON ((19.56588 41.8613, 19.56588 42.04096,...  19.327275  41.861301   \n",
       "38  POLYGON ((19.80449 39.88502, 19.80449 40.06468...  19.565883  39.885016   \n",
       "39  POLYGON ((19.80449 40.06468, 19.80449 40.24434...  19.565883  40.064678   \n",
       "47  POLYGON ((19.80449 41.50198, 19.80449 41.68164...  19.565883  41.501976   \n",
       "48  POLYGON ((19.80449 41.68164, 19.80449 41.8613,...  19.565883  41.681639   \n",
       "54  POLYGON ((20.0431 39.52569, 20.0431 39.70535, ...  19.804491  39.525692   \n",
       "55  POLYGON ((20.0431 39.70535, 20.0431 39.88502, ...  19.804491  39.705354   \n",
       "56  POLYGON ((20.0431 39.88502, 20.0431 40.06468, ...  19.804491  39.885016   \n",
       "\n",
       "      max_lon    max_lat  codeMaille20Km country_code  \n",
       "4   19.327275  40.424003  20kmE1920N4033          ALB  \n",
       "5   19.327275  40.603665  20kmE1920N4051          ALB  \n",
       "6   19.327275  40.783327  20kmE1920N4069          ALB  \n",
       "21  19.565883  40.244341  20kmE1944N4015          ALB  \n",
       "22  19.565883  40.424003  20kmE1944N4033          ALB  \n",
       "23  19.565883  40.603665  20kmE1944N4051          ALB  \n",
       "24  19.565883  40.783327  20kmE1944N4069          ALB  \n",
       "25  19.565883  40.962990  20kmE1944N4087          ALB  \n",
       "26  19.565883  41.142652  20kmE1944N4105          ALB  \n",
       "27  19.565883  41.322314  20kmE1944N4123          ALB  \n",
       "28  19.565883  41.501976  20kmE1944N4141          ALB  \n",
       "29  19.565883  41.681639  20kmE1944N4159          ALB  \n",
       "30  19.565883  41.861301  20kmE1944N4177          ALB  \n",
       "31  19.565883  42.040963  20kmE1944N4195          ALB  \n",
       "38  19.804491  40.064678  20kmE1968N3997          ALB  \n",
       "39  19.804491  40.244341  20kmE1968N4015          ALB  \n",
       "47  19.804491  41.681639  20kmE1968N4159          ALB  \n",
       "48  19.804491  41.861301  20kmE1968N4177          ALB  \n",
       "54  20.043100  39.705354  20kmE1992N3961          ALB  \n",
       "55  20.043100  39.885016  20kmE1992N3979          ALB  \n",
       "56  20.043100  40.064678  20kmE1992N3997          ALB  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Génération de la grille pour le pays choisi\n",
    "country = world_terrestre[world_terrestre[\"color_code\"] == country_code]\n",
    "country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
    "\n",
    "# Get the bounds of the input GeoDataFrame\n",
    "minx, miny, maxx, maxy = country_geometry.bounds\n",
    "\n",
    "# Step 3: Adjust bounding box to align with (0, 0)\n",
    "# Use approximate degrees per km for the middle of the country\n",
    "midpoint_lat = (miny + maxy) / 2\n",
    "\n",
    "country_grid_terrestre = create_country_grid_WGS84(world_terrestre, country_code,\"color_code\",grid_size_km=grid_size_km,midpoint_lat=midpoint_lat,display=False)\n",
    "nouveaux_noms_columns = {'cell_name': cle_geo}\n",
    "country_grid_terrestre = country_grid_terrestre.rename(columns=nouveaux_noms_columns)\n",
    "\n",
    "country_maritime = world_maritime[world_maritime[\"ISO_TER1\"] == country_code]\n",
    "\n",
    "country_grid_maritime = gpd.GeoDataFrame()\n",
    "if not country_maritime.empty:\n",
    "    country_grid_maritime = create_country_grid_WGS84(world_maritime, country_code,\"ISO_TER1\" ,grid_size_km=grid_size_km,midpoint_lat=midpoint_lat,display=False)\n",
    "    nouveaux_noms_columns = {'cell_name': cle_geo}\n",
    "    country_grid_maritime = country_grid_maritime.rename(columns=nouveaux_noms_columns)\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_grid = pd.concat([country_grid_maritime, country_grid_terrestre])\n",
    "\n",
    "# Check for duplicates based on all columns\n",
    "duplicates = combined_grid[combined_grid.duplicated(subset=[cle_geo])]\n",
    "combined_grid_unique = combined_grid.drop_duplicates(subset=[cle_geo])\n",
    "\n",
    "# Display duplicates\n",
    "print(\"Doublons trouvés :\")\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc83a7fb-36a0-488a-8a88-cb21b00860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "country_name='Albania'\n",
    "output_path = os.path.join(\n",
    "    \"C:\\\\Users\\\\anormand\\\\Documents\\\\Projet Python\\\\Biodiv\\\\Data\\\\GBIF_\"+\n",
    "    country_name.replace(' ', '_'),\n",
    "    \"SIG\",\n",
    "    f\"country_grid_terrestre_{grid_size_km}km.geojson\"\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create missing directories\n",
    "\n",
    "country_grid_terrestre.to_file(output_path, driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e952724-6c26-4112-8546-0ca1d726986b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  country_grid_terrestre.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_terrestre_\"+str(grid_size_km)+\"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\" + country_name.replace(' ', '_') + \"\\SIG\\country_grid_maritime_\" + str(grid_size_km) + \"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  combined_grid_unique.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_combined_\"+str(grid_size_km)+\"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  country_grid_terrestre.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_terrestre_\"+str(grid_size_km)+\"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:6: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\" + country_name.replace(' ', '_') + \"\\SIG\\country_grid_maritime_\" + str(grid_size_km) + \"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4074169326.py:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  combined_grid_unique.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_combined_\"+str(grid_size_km)+\"km.geojson\",\n"
     ]
    },
    {
     "ename": "DataSourceError",
     "evalue": "Failed to create GeoJSON datasource: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\_io.pyx:1936\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\_err.pyx:183\u001b[0m, in \u001b[0;36mpyogrio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: Failed to create GeoJSON datasource: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sauvegarder le fichier grille\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcountry_grid_terrestre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43manormand\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProjet Python\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mBiodiv\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mGBIF_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcountry_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSIG\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcountry_grid_terrestre_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrid_size_km\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkm.geojson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeoJSON\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m country_grid_maritime\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m     country_grid_maritime\u001b[38;5;241m.\u001b[39mto_file(\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manormand\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjet Python\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBiodiv\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGBIF_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m country_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSIG\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcountry_grid_maritime_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grid_size_km) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkm.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m         driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoJSON\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\geodataframe.py:1536\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[1;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \n\u001b[0;32m   1443\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1532\u001b[0m \n\u001b[0;32m   1533\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[1;32m-> 1536\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\io\\file.py:686\u001b[0m, in \u001b[0;36m_to_file\u001b[1;34m(df, filename, driver, schema, index, mode, crs, engine, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 686\u001b[0m     \u001b[43m_to_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    688\u001b[0m     _to_file_fiona(df, filename, driver, schema, crs, mode, metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\geopandas\\io\\file.py:748\u001b[0m, in \u001b[0;36m_to_file_pyogrio\u001b[1;34m(df, filename, driver, schema, crs, mode, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoDataFrame cannot contain duplicated column names.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\geopandas.py:654\u001b[0m, in \u001b[0;36mwrite_dataframe\u001b[1;34m(df, path, layer, driver, encoding, geometry_type, promote_to_multi, nan_as_null, append, use_arrow, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    652\u001b[0m     geometry \u001b[38;5;241m=\u001b[39m to_wkb(geometry\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m--> 654\u001b[0m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\raw.py:709\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(path, geometry, field_data, fields, field_mask, layer, driver, geometry_type, crs, encoding, promote_to_multi, nan_as_null, append, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, gdal_tz_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# preprocess kwargs and split in dataset and layer creation options\u001b[39;00m\n\u001b[0;32m    705\u001b[0m dataset_kwargs, layer_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_kwargs(\n\u001b[0;32m    706\u001b[0m     driver, dataset_options, layer_options, kwargs\n\u001b[0;32m    707\u001b[0m )\n\u001b[1;32m--> 709\u001b[0m \u001b[43mogr_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\_io.pyx:2298\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_write\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\_io.pyx:2125\u001b[0m, in \u001b[0;36mpyogrio._io.create_ogr_dataset_layer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\_io.pyx:1945\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDataSourceError\u001b[0m: Failed to create GeoJSON datasource: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_Albania\\SIG\\country_grid_terrestre_20km.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le fichier grille\n",
    "country_grid_terrestre.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_terrestre_\"+str(grid_size_km)+\"km.geojson\", \n",
    "                             driver=\"GeoJSON\")\n",
    "if not country_grid_maritime.empty:\n",
    "    country_grid_maritime.to_file(\n",
    "        r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\" + country_name.replace(' ', '_') + \"\\SIG\\country_grid_maritime_\" + str(grid_size_km) + \"km.geojson\", \n",
    "        driver=\"GeoJSON\"\n",
    "    )\n",
    "combined_grid_unique.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_combined_\"+str(grid_size_km)+\"km.geojson\", \n",
    "                             driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "916da911-9f9e-4012-b8d2-423e8b563d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Importer les données de biodiversité du pays ###\n",
    "cle_ID='speciesKey'\n",
    "cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "colonnes_a_importer=['kingdom','phylum','class','order','family','genus','species','verbatimScientificName',\n",
    "                         'taxonRank','countryCode','occurrenceStatus', 'individualCount', \n",
    "                     'decimalLongitude','decimalLatitude','eventDate','taxonKey','speciesKey','occurrenceID', 'year']\n",
    "fichier = \"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')+\"/Raw/extractGBIF_\"+country_name.replace(' ', '_')+\"_12112024.csv\"  # Remplace par le chemin vers ton fichier\n",
    "\n",
    "df_biodiv = pd.read_csv(fichier, sep='\\t',on_bad_lines='skip',usecols=colonnes_a_importer,nrows=4572978 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a486af-523c-457c-b515-4344857e1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurrenceID               9740\n",
      "kingdom                       0\n",
      "phylum                      726\n",
      "class                      2079\n",
      "order                      3420\n",
      "family                      990\n",
      "genus                      1864\n",
      "species                    5765\n",
      "taxonRank                     0\n",
      "verbatimScientificName     1055\n",
      "countryCode                   0\n",
      "occurrenceStatus              0\n",
      "individualCount           72315\n",
      "decimalLatitude           19732\n",
      "decimalLongitude          19732\n",
      "eventDate                 53470\n",
      "year                      15915\n",
      "taxonKey                      0\n",
      "speciesKey                 5767\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de NaN par colonne\n",
    "df_biodiv['eventDate'] = pd.to_datetime(df_biodiv['eventDate'], errors='coerce', utc=True)\n",
    "\n",
    "df_biodiv['year'] = df_biodiv['year'].fillna(df_biodiv['eventDate'].dt.year)\n",
    "# Compter le nombre de NaN par colonne\n",
    "nan_counts = df_biodiv.isna().sum()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ded2cebf-c42d-40ec-af50-04f3bba4e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>taxonRank</th>\n",
       "      <th>verbatimScientificName</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>occurrenceStatus</th>\n",
       "      <th>individualCount</th>\n",
       "      <th>decimalLatitude</th>\n",
       "      <th>decimalLongitude</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>year</th>\n",
       "      <th>taxonKey</th>\n",
       "      <th>speciesKey</th>\n",
       "      <th>grid_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>URN:catalog:CLO:EBIRD:OBS1687445869</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Aves</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Hirundinidae</td>\n",
       "      <td>Hirundo</td>\n",
       "      <td>Hirundo rustica</td>\n",
       "      <td>SPECIES</td>\n",
       "      <td>Hirundo rustica</td>\n",
       "      <td>AL</td>\n",
       "      <td>PRESENT</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40.963840</td>\n",
       "      <td>19.469254</td>\n",
       "      <td>2023-04-16 00:00:00+00:00</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>9515886</td>\n",
       "      <td>9515886.0</td>\n",
       "      <td>20kmE1944N4105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URN:catalog:CLO:EBIRD:OBS998242125</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Aves</td>\n",
       "      <td>Suliformes</td>\n",
       "      <td>Phalacrocoracidae</td>\n",
       "      <td>Phalacrocorax</td>\n",
       "      <td>Phalacrocorax carbo</td>\n",
       "      <td>SPECIES</td>\n",
       "      <td>Phalacrocorax carbo</td>\n",
       "      <td>AL</td>\n",
       "      <td>PRESENT</td>\n",
       "      <td>8.0</td>\n",
       "      <td>40.927130</td>\n",
       "      <td>19.496225</td>\n",
       "      <td>2020-10-18 00:00:00+00:00</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2481890</td>\n",
       "      <td>2481890.0</td>\n",
       "      <td>20kmE1944N4087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URN:catalog:CLO:EBIRD:OBS1149393329</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Aves</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Motacillidae</td>\n",
       "      <td>Motacilla</td>\n",
       "      <td>Motacilla cinerea</td>\n",
       "      <td>SPECIES</td>\n",
       "      <td>Motacilla cinerea</td>\n",
       "      <td>AL</td>\n",
       "      <td>PRESENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.313248</td>\n",
       "      <td>19.824556</td>\n",
       "      <td>2021-05-08 00:00:00+00:00</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2490310</td>\n",
       "      <td>2490310.0</td>\n",
       "      <td>20kmE1992N4123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URN:catalog:CLO:EBIRD:OBS861391501</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Aves</td>\n",
       "      <td>Anseriformes</td>\n",
       "      <td>Anatidae</td>\n",
       "      <td>Mareca</td>\n",
       "      <td>Mareca penelope</td>\n",
       "      <td>SPECIES</td>\n",
       "      <td>Mareca penelope</td>\n",
       "      <td>AL</td>\n",
       "      <td>PRESENT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.882507</td>\n",
       "      <td>19.446424</td>\n",
       "      <td>2020-02-08 00:00:00+00:00</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>8000602</td>\n",
       "      <td>8000602.0</td>\n",
       "      <td>20kmE1944N4087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>URN:catalog:CLO:EBIRD:OBS1492950810</td>\n",
       "      <td>Animalia</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Aves</td>\n",
       "      <td>Passeriformes</td>\n",
       "      <td>Paridae</td>\n",
       "      <td>Parus</td>\n",
       "      <td>Parus major</td>\n",
       "      <td>SPECIES</td>\n",
       "      <td>Parus major</td>\n",
       "      <td>AL</td>\n",
       "      <td>PRESENT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42.458850</td>\n",
       "      <td>19.874212</td>\n",
       "      <td>2021-08-17 00:00:00+00:00</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>9705453</td>\n",
       "      <td>9705453.0</td>\n",
       "      <td>20kmE1992N4249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          occurrenceID   kingdom    phylum class  \\\n",
       "0  URN:catalog:CLO:EBIRD:OBS1687445869  Animalia  Chordata  Aves   \n",
       "1   URN:catalog:CLO:EBIRD:OBS998242125  Animalia  Chordata  Aves   \n",
       "2  URN:catalog:CLO:EBIRD:OBS1149393329  Animalia  Chordata  Aves   \n",
       "3   URN:catalog:CLO:EBIRD:OBS861391501  Animalia  Chordata  Aves   \n",
       "4  URN:catalog:CLO:EBIRD:OBS1492950810  Animalia  Chordata  Aves   \n",
       "\n",
       "           order             family          genus              species  \\\n",
       "0  Passeriformes       Hirundinidae        Hirundo      Hirundo rustica   \n",
       "1     Suliformes  Phalacrocoracidae  Phalacrocorax  Phalacrocorax carbo   \n",
       "2  Passeriformes       Motacillidae      Motacilla    Motacilla cinerea   \n",
       "3   Anseriformes           Anatidae         Mareca      Mareca penelope   \n",
       "4  Passeriformes            Paridae          Parus          Parus major   \n",
       "\n",
       "  taxonRank verbatimScientificName countryCode occurrenceStatus  \\\n",
       "0   SPECIES        Hirundo rustica          AL          PRESENT   \n",
       "1   SPECIES    Phalacrocorax carbo          AL          PRESENT   \n",
       "2   SPECIES      Motacilla cinerea          AL          PRESENT   \n",
       "3   SPECIES        Mareca penelope          AL          PRESENT   \n",
       "4   SPECIES            Parus major          AL          PRESENT   \n",
       "\n",
       "   individualCount  decimalLatitude  decimalLongitude  \\\n",
       "0             12.0        40.963840         19.469254   \n",
       "1              8.0        40.927130         19.496225   \n",
       "2              1.0        41.313248         19.824556   \n",
       "3              4.0        40.882507         19.446424   \n",
       "4              2.0        42.458850         19.874212   \n",
       "\n",
       "                  eventDate    year  taxonKey  speciesKey       grid_name  \n",
       "0 2023-04-16 00:00:00+00:00  2023.0   9515886   9515886.0  20kmE1944N4105  \n",
       "1 2020-10-18 00:00:00+00:00  2020.0   2481890   2481890.0  20kmE1944N4087  \n",
       "2 2021-05-08 00:00:00+00:00  2021.0   2490310   2490310.0  20kmE1992N4123  \n",
       "3 2020-02-08 00:00:00+00:00  2020.0   8000602   8000602.0  20kmE1944N4087  \n",
       "4 2021-08-17 00:00:00+00:00  2021.0   9705453   9705453.0  20kmE1992N4249  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Associer chaque observation à une maille de la grille\n",
    "\n",
    "# Assurez-vous que 'decimalLongitude' et 'decimalLatitude' sont des colonnes numériques\n",
    "df_biodiv['decimalLongitude'] = pd.to_numeric(df_biodiv['decimalLongitude'], errors='coerce')\n",
    "df_biodiv['decimalLatitude'] = pd.to_numeric(df_biodiv['decimalLatitude'], errors='coerce')\n",
    "\n",
    "# Filtrer les lignes où les coordonnées sont NaN\n",
    "df_biodiv = df_biodiv.dropna(subset=['decimalLongitude', 'decimalLatitude',cle_ID]).reset_index(drop=True)\n",
    "\n",
    "# Ajouter le grid aux points\n",
    "df_biodiv_with_grid = add_grid_to_country(df_biodiv, combined_grid_unique,cle_geo)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "df_biodiv_with_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd669b55-538b-4cd2-9bf3-caef0c26bd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En entrée : nombre d'espèces observées : 7184\n",
      "En entrée : nombre d'obs : 103579\n",
      "En sortie : nombre d'espèces observées :7004 soit une perte de 3%\n",
      "En sortie : nombre d'obs :102406 soit une perte de 1 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\4107726356.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['individualCount'].fillna(1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Pré-traitement des données\n",
    "\n",
    "# Lire uniquement les colonnes spécifiées du fichier dans un DataFrame\n",
    "df=df_biodiv_with_grid.copy()\n",
    "\n",
    "cle_ID='speciesKey'\n",
    "cle_date='year'\n",
    "cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "\n",
    "n_especes_entrée=len(df[cle_ID].unique())\n",
    "n_obs_entrée=len(df)\n",
    "print(\"En entrée : nombre d'espèces observées :\",n_especes_entrée)\n",
    "print(\"En entrée : nombre d'obs :\",n_obs_entrée)\n",
    "\n",
    "colonnes_obligatoires=[cle_ID]\n",
    "# Supprimer les lignes ou une donnée cruciale manque\n",
    "df_cleaned = df.dropna(subset=colonnes_obligatoires)\n",
    "\n",
    "# Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "#valid_ranks = ['SPECIES', 'SUBSPECIES', 'VARIETY']\n",
    "#df_cleaned = df_cleaned[df_cleaned['taxonRank'].isin(valid_ranks)].reset_index(drop=True)\n",
    "\n",
    "# Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "df_cleaned = df_cleaned[df_cleaned['occurrenceStatus']=='PRESENT'].reset_index(drop=True)\n",
    "\n",
    "df_cleaned[cle_ID] = df_cleaned[cle_ID].astype(int)\n",
    "\n",
    "df_cleaned.rename(columns={'grid_name': cle_geo}, inplace=True)\n",
    "\n",
    "# Ajouter des colonnes year, month, day et day_of_year\n",
    "#df_cleaned[cle_date] = pd.to_datetime(df_cleaned[cle_date], utc=True, errors='coerce')\n",
    "\n",
    "# Now safely access .dt.year, .dt.month, etc., from df_cleaned\n",
    "#df_cleaned['year'] = df_cleaned[cle_date].dt.year\n",
    "\n",
    "# Convertir 'individualCount' en numérique, en forçant les erreurs à NaN\n",
    "df_cleaned['individualCount'] = pd.to_numeric(df_cleaned['individualCount'], errors='coerce')\n",
    "\n",
    "# Remplacer les NaN par 1\n",
    "df_cleaned['individualCount'].fillna(1, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"En sortie : nombre d'espèces observées :{len(df_cleaned[cle_ID].unique())} soit une perte de {100-(round(len(df_cleaned[cle_ID].unique())/n_especes_entrée*100))}%\")\n",
    "print(f\"En sortie : nombre d'obs :{len(df_cleaned)} soit une perte de {100-round(len(df_cleaned)/n_obs_entrée*100)} %\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5138c-d811-4855-91d6-f262c347f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier pré-traité sous le format 1 ligne = 1 observation\n",
    "\"\"\"\n",
    "df_cleaned.to_csv(\"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+\n",
    "                  country_name.replace(' ', '_')+\n",
    "                  \"/Filtered\"+\n",
    "                  \"/data_GBIF_\"+country_name.replace(' ', '_')+\"_\"+str(grid_size_km)+\"Km_filtered.csv\",\n",
    "                  index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb65f7ac-f17b-4b51-855d-4cf9fafd5357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "periode\n",
      "Période 3: 2011 à 2024    91506\n",
      "Période 2: 1991 à 2010     4075\n",
      "Période 1: 1801 à 1990     1623\n",
      "Name: count, dtype: int64\n",
      "En sortie : nombre d'obs :97245 soit une perte de 6 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:169: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'periode'] = pd.cut(df['year'], bins=bornes_temporelles,\n"
     ]
    }
   ],
   "source": [
    "# Grouper les données par maille et par période \n",
    "annee_mini=1\n",
    "bornes_temporelles=[1800, 1990,2010, 2024] \n",
    "\n",
    "df_maille_espece=formater_maille_espece_GBIF(df_cleaned,cle_geo,cle_ID,annee_mini,bornes_temporelles)\n",
    "\n",
    "print(f\"En sortie : nombre d'obs :{len(df_cleaned[df_cleaned['year']>annee_mini])} soit une perte de {100-round(len(df_cleaned[df_cleaned['year']>annee_mini])/n_obs_entrée*100)} %\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e9a9251-e970-4aa6-8d0e-cb9e9263ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_noms_vernaculaires_merged = pd.read_csv(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\TAXO_GBIF\\dico_noms_vernaculaires_merged.csv\")\n",
    "\n",
    "df_maille_espece=pd.merge(df_maille_espece,dico_noms_vernaculaires_merged,on=cle_ID,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1364e3e8-53b3-4528-bd31-84c276e4e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les données groupées\n",
    "\n",
    "colonnes_a_conserver = ['kingdom','phylum','class','order','family','genus','species',\n",
    "                     'nombreObs',cle_ID,cle_geo,'periode','taxonRank','vernacularName_fr','vernacularName_en','occurrenceID']\n",
    "df_to_save=df_maille_espece[colonnes_a_conserver]\n",
    "df_to_save.to_csv(\"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')\n",
    "                  +\"/data_GBIF_\"+country_name.replace(' ', '_')+'_'+cle_geo+\"_periodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d972664-cf20-4dc9-9cc2-4ac20c679e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:47: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:40: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  country_grid_terrestre.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_terrestre_\"+str(grid_size_km)+\"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:44: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\" + country_name.replace(' ', '_') + \"\\SIG\\country_grid_maritime_\" + str(grid_size_km) + \"km.geojson\",\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:47: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  combined_grid_unique.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_combined_\"+str(grid_size_km)+\"km.geojson\",\n"
     ]
    }
   ],
   "source": [
    "def formater_data_gbif_all_in_one(country_name,grid_size_km,cle_ID='speciesKey',annee_mini=0,bornes_temporelles=[1800, 1990,2010, 2024]):\n",
    "    \n",
    "    world_terrestre = gpd.read_file(path+\"/SIG_global/world-administrative-boundaries.geojson\")\n",
    "    world_maritime = gpd.read_file(path+\"/SIG_global/eez_v11.gpkg\")\n",
    "    \n",
    "    cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "    country_code = world_terrestre[world_terrestre[\"name\"] == country_name][\"color_code\"].iloc[0]\n",
    "    \n",
    "    # Génération de la grille pour le pays choisi\n",
    "    country = world_terrestre[world_terrestre[\"color_code\"] == country_code]\n",
    "    country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
    "    \n",
    "    # Get the bounds of the input GeoDataFrame\n",
    "    minx, miny, maxx, maxy = country_geometry.bounds\n",
    "    \n",
    "    # Step 3: Adjust bounding box to align with (0, 0)\n",
    "    # Use approximate degrees per km for the middle of the country\n",
    "    midpoint_lat = (miny + maxy) / 2\n",
    "    \n",
    "    country_grid_terrestre = create_country_grid_WGS84(world_terrestre, country_code,\"color_code\",grid_size_km=grid_size_km,midpoint_lat=midpoint_lat)\n",
    "    nouveaux_noms_columns = {'cell_name': cle_geo}\n",
    "    country_grid_terrestre = country_grid_terrestre.rename(columns=nouveaux_noms_columns)\n",
    "    \n",
    "    country_maritime = world_maritime[world_maritime[\"ISO_TER1\"] == country_code]\n",
    "    \n",
    "    country_grid_maritime = gpd.GeoDataFrame()\n",
    "    if not country_maritime.empty:\n",
    "        country_grid_maritime = create_country_grid_WGS84(world_maritime, country_code,\"ISO_TER1\" ,grid_size_km=grid_size_km,midpoint_lat=midpoint_lat)\n",
    "        nouveaux_noms_columns = {'cell_name': cle_geo}\n",
    "        country_grid_maritime = country_grid_maritime.rename(columns=nouveaux_noms_columns)\n",
    "    \n",
    "    # Concatenate the two DataFrames\n",
    "    combined_grid = pd.concat([country_grid_maritime, country_grid_terrestre])\n",
    "    \n",
    "    # Check for duplicates based on all columns\n",
    "    duplicates = combined_grid[combined_grid.duplicated(subset=[cle_geo])]\n",
    "    combined_grid_unique = combined_grid.drop_duplicates(subset=[cle_geo])\n",
    "\n",
    "    # Sauvegarder le fichier grille\n",
    "    country_grid_terrestre.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_terrestre_\"+str(grid_size_km)+\"km.geojson\", \n",
    "                                 driver=\"GeoJSON\")\n",
    "    if not country_grid_maritime.empty:\n",
    "        country_grid_maritime.to_file(\n",
    "            r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\" + country_name.replace(' ', '_') + \"\\SIG\\country_grid_maritime_\" + str(grid_size_km) + \"km.geojson\", \n",
    "            driver=\"GeoJSON\"\n",
    "        )\n",
    "    combined_grid_unique.to_file(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_\"+country_name.replace(' ', '_')+\"\\SIG\\country_grid_combined_\"+str(grid_size_km)+\"km.geojson\", \n",
    "                                 driver=\"GeoJSON\")\n",
    "    \n",
    "    ### Importer les données de biodiversité du pays ###\n",
    "    \n",
    "    cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "    \n",
    "    fichier = \"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')+\"/Raw/extractGBIF_\"+country_name.replace(' ', '_')+\"_12112024.csv\"  # Remplace par le chemin vers ton fichier\n",
    "    \n",
    "    colonnes_a_importer=['kingdom','phylum','class','order','family','genus','species','verbatimScientificName',\n",
    "                         'taxonRank','countryCode','occurrenceStatus', 'individualCount', 'decimalLongitude','decimalLatitude','eventDate','taxonKey','speciesKey','occurrenceID', 'year'\n",
    "                         ]\n",
    "    \n",
    "    df_biodiv = pd.read_csv(\n",
    "        fichier, \n",
    "        sep='\\t', \n",
    "        on_bad_lines='skip',  # Skip problematic lines\n",
    "        quoting=3,            # Ignore quotes\n",
    "        usecols=colonnes_a_importer\n",
    "        )\n",
    "    \n",
    "    df_biodiv[cle_ID] = pd.to_numeric(df_biodiv[cle_ID], errors='coerce')\n",
    "    \n",
    "    # Associer chaque observation à une maille de la grille\n",
    "    \n",
    "    # Assurez-vous que 'decimalLongitude' et 'decimalLatitude' sont des colonnes numériques\n",
    "    df_biodiv['decimalLongitude'] = pd.to_numeric(df_biodiv['decimalLongitude'], errors='coerce')\n",
    "    df_biodiv['decimalLatitude'] = pd.to_numeric(df_biodiv['decimalLatitude'], errors='coerce')\n",
    "    \n",
    "    # Filtrer les lignes où les coordonnées sont NaN\n",
    "    df_biodiv = df_biodiv.dropna(subset=['decimalLongitude', 'decimalLatitude',cle_ID]).reset_index(drop=True)\n",
    "    \n",
    "    # Ajouter le grid aux points\n",
    "    df_biodiv_with_grid = add_grid_to_country(df_biodiv, combined_grid_unique,cle_geo)\n",
    "    \n",
    "    # Pré-traitement des données\n",
    "    \n",
    "    # Lire uniquement les colonnes spécifiées du fichier dans un DataFrame\n",
    "    df=df_biodiv_with_grid.copy()\n",
    "    \n",
    "    cle_date='year'\n",
    "    \n",
    "    colonnes_obligatoires=[cle_ID]\n",
    "    # Supprimer les lignes ou une donnée cruciale manque\n",
    "    df_cleaned = df.dropna(subset=colonnes_obligatoires)\n",
    "    \n",
    "    # Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "    #valid_ranks = ['SPECIES', 'SUBSPECIES', 'VARIETY']\n",
    "    #df_cleaned = df_cleaned[df_cleaned['taxonRank'].isin(valid_ranks)].reset_index(drop=True)\n",
    "    \n",
    "    # Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "    df_cleaned = df_cleaned[df_cleaned['occurrenceStatus']=='PRESENT'].reset_index(drop=True)\n",
    "    \n",
    "    df_cleaned[cle_ID] = df_cleaned[cle_ID].astype(int)\n",
    "    \n",
    "    df_cleaned.rename(columns={'grid_name': cle_geo}, inplace=True)\n",
    "    \n",
    "    df_cleaned['individualCount'] = df_cleaned['individualCount'].fillna(1)\n",
    "    \n",
    "    # Convertir 'individualCount' en numérique, en forçant les erreurs à NaN\n",
    "    df_cleaned['individualCount'] = pd.to_numeric(df_cleaned['individualCount'], errors='coerce')\n",
    "    \n",
    "    # Remplacer les NaN par 1\n",
    "    df_cleaned['individualCount'].fillna(1, inplace=True)\n",
    "    \n",
    "    # Grouper les données par maille et par période \n",
    "    \n",
    "    df_maille_espece=formater_maille_espece_GBIF(df_cleaned,cle_geo,cle_ID,annee_mini,bornes_temporelles)\n",
    "    \n",
    "    dico_noms_vernaculaires_merged = pd.read_csv(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\TAXO_GBIF\\dico_noms_vernaculaires_merged.csv\")\n",
    "    \n",
    "    df_maille_espece=pd.merge(df_maille_espece,dico_noms_vernaculaires_merged,on=cle_ID,how=\"left\")\n",
    "    \n",
    "    # Sauvegarder les données groupées\n",
    "    \n",
    "    colonnes_a_conserver = ['kingdom','phylum','class','order','family','genus','species',\n",
    "                         'nombreObs',cle_ID,cle_geo,'periode','taxonRank','vernacularName_fr','vernacularName_en','occurrenceID']\n",
    "    df_to_save=df_maille_espece[colonnes_a_conserver]\n",
    "    df_to_save.to_csv(\"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')\n",
    "                      +\"/data_GBIF_\"+country_name.replace(' ', '_')+'_'+cle_geo+\"_periodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4621bbc6-b47b-4412-80fd-719a586d7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madagascar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:11: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:110: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['individualCount'].fillna(1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "periode\n",
      "Période 3: 2011 à 2024    653149\n",
      "Période 2: 1991 à 2010    535246\n",
      "Période 1: 1801 à 1990    222796\n",
      "Name: count, dtype: int64\n",
      "Madagascar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:11: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:44: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  country_geometry = country.geometry.unary_union  # Combine all geometries into one\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\3373840906.py:95: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  grid = grid[grid.intersects(country.unary_union)]\n",
      "C:\\Users\\anormand\\AppData\\Local\\Temp\\ipykernel_54016\\7265988.py:110: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['individualCount'].fillna(1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "periode\n",
      "Période 3: 2011 à 2024    653149\n",
      "Période 2: 1991 à 2010    535246\n",
      "Période 1: 1801 à 1990    222796\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "grid_size_km=[10,20]\n",
    "\n",
    "for size_km in grid_size_km:\n",
    "    countries=['Madagascar'] #20\n",
    "\n",
    "    for country in countries:\n",
    "        print(country)\n",
    "        formater_data_gbif_all_in_one(country,size_km,annee_mini=0,bornes_temporelles=[1800, 1990,2010, 2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a51aad-9cdc-4130-b129-f7f5ea0689be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_ID='speciesKey'\n",
    "grid_size_km=20\n",
    "cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "country_name=\"France\"\n",
    "fichier=r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_France\\Raw\\extractGBIF_France_12112024.csv\"\n",
    "# Taille de chaque paquet\n",
    "lines_per_chunk = 10000000\n",
    "\n",
    "colonnes_a_importer=['kingdom','phylum','class','order','family','genus','species','verbatimScientificName',\n",
    "                     'taxonRank','countryCode','occurrenceStatus', 'individualCount', 'decimalLongitude','decimalLatitude','eventDate','taxonKey','speciesKey','occurrenceID', 'year']\n",
    "# Lecture et découpage du fichier\n",
    "chunk_number = 0\n",
    "\n",
    "for df_biodiv in pd.read_csv(fichier, sep='\\t', chunksize=lines_per_chunk, on_bad_lines='skip', usecols=colonnes_a_importer):\n",
    "    chunk_number += 1\n",
    "    # Assurez-vous que 'decimalLongitude' et 'decimalLatitude' sont des colonnes numériques\n",
    "    df_biodiv['decimalLongitude'] = pd.to_numeric(df_biodiv['decimalLongitude'], errors='coerce')\n",
    "    df_biodiv['decimalLatitude'] = pd.to_numeric(df_biodiv['decimalLatitude'], errors='coerce')\n",
    "    df_biodiv[cle_ID] = pd.to_numeric(df_biodiv[cle_ID], errors='coerce')\n",
    "    # Filtrer les lignes où les coordonnées sont NaN\n",
    "    df_biodiv = df_biodiv.dropna(subset=['decimalLongitude', 'decimalLatitude',cle_ID]).reset_index(drop=True)\n",
    "    \n",
    "    # Ajouter le grid aux points\n",
    "    df_biodiv_with_grid = add_grid_to_country(df_biodiv, combined_grid_unique,cle_geo)\n",
    "    \n",
    "    # Afficher les premières lignes\n",
    "    df_biodiv_with_grid.head()\n",
    "\n",
    "    # Pré-traitement des données\n",
    "    # Lire uniquement les colonnes spécifiées du fichier dans un DataFrame\n",
    "    df=df_biodiv_with_grid.copy()\n",
    "    \n",
    "    cle_date='year'\n",
    "\n",
    "    n_especes_entrée=len(df[cle_ID].unique())\n",
    "    n_obs_entrée=len(df)\n",
    "    print(\"En entrée : nombre d'espèces observées :\",n_especes_entrée)\n",
    "    print(\"En entrée : nombre d'obs :\",n_obs_entrée)\n",
    "    \n",
    "    colonnes_obligatoires=[cle_ID]\n",
    "    # Supprimer les lignes ou une donnée cruciale manque\n",
    "    df_cleaned = df.dropna(subset=colonnes_obligatoires)\n",
    "    \n",
    "    # Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "    #valid_ranks = ['SPECIES', 'SUBSPECIES', 'VARIETY']\n",
    "    #df_cleaned = df_cleaned[df_cleaned['taxonRank'].isin(valid_ranks)].reset_index(drop=True)\n",
    "    \n",
    "    # Filtrer les lignes où 'taxonRank' est SPECIES, SUBSPECIES ou VARIETY\n",
    "    df_cleaned = df_cleaned[df_cleaned['occurrenceStatus']=='PRESENT'].reset_index(drop=True)\n",
    "    \n",
    "    df_cleaned[cle_ID] = df_cleaned[cle_ID].astype(int)\n",
    "    \n",
    "    df_cleaned.rename(columns={'grid_name': cle_geo}, inplace=True)\n",
    "    \n",
    "    df_cleaned['individualCount'] = df_cleaned['individualCount'].fillna(1)\n",
    "    \n",
    "    # Ajouter des colonnes year, month, day et day_of_year\n",
    "    #df_cleaned[cle_date] = pd.to_datetime(df_cleaned[cle_date], utc=True, errors='coerce')\n",
    "    \n",
    "    # Now safely access .dt.year, .dt.month, etc., from df_cleaned\n",
    "    #df_cleaned['year'] = df_cleaned[cle_date].dt.year\n",
    "    \n",
    "    # Convertir 'individualCount' en numérique, en forçant les erreurs à NaN\n",
    "    df_cleaned['individualCount'] = pd.to_numeric(df_cleaned['individualCount'], errors='coerce')\n",
    "    \n",
    "    # Remplacer les NaN par 1\n",
    "    df_cleaned['individualCount'].fillna(1, inplace=True)\n",
    "    \n",
    "    \n",
    "    print(f\"En sortie : nombre d'espèces observées :{len(df_cleaned[cle_ID].unique())} soit une perte de {100-(round(len(df_cleaned[cle_ID].unique())/n_especes_entrée*100))}%\")\n",
    "    print(f\"En sortie : nombre d'obs :{len(df_cleaned)} soit une perte de {100-round(len(df_cleaned)/n_obs_entrée*100)} %\") \n",
    "    \n",
    "    # Grouper les données par maille et par période \n",
    "    annee_mini=0\n",
    "    bornes_temporelles=[1800, 1990,2010, 2024] \n",
    "    \n",
    "    df_maille_espece=formater_maille_espece_GBIF(df_cleaned,cle_geo,cle_ID,annee_mini,bornes_temporelles)\n",
    "    \n",
    "    print(f\"En sortie : nombre d'obs :{len(df_cleaned[df_cleaned['year']>annee_mini])} soit une perte de {100-round(len(df_cleaned[df_cleaned['year']>annee_mini])/n_obs_entrée*100)} %\") \n",
    "    \n",
    "    dico_noms_vernaculaires_merged = pd.read_csv(r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\TAXO_GBIF\\dico_noms_vernaculaires_merged.csv\")\n",
    "    \n",
    "    df_maille_espece=pd.merge(df_maille_espece,dico_noms_vernaculaires_merged,on=cle_ID,how=\"left\")\n",
    "    \n",
    "    # Sauvegarder les données groupées\n",
    "    \n",
    "    colonnes_a_conserver = ['kingdom','phylum','class','order','family','genus','species',\n",
    "                         'nombreObs',cle_ID,cle_geo,'periode','taxonRank','vernacularName_fr','vernacularName_en','occurrenceID']\n",
    "    df_to_save=df_maille_espece[colonnes_a_conserver]\n",
    "    df_to_save.to_csv(\"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')\n",
    "                      +\"/data_GBIF_\"+country_name.replace(' ', '_')+'_'+cle_geo+\"_periodes_\"+str(chunk_number)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edebfb-5702-44a3-83d8-956cdfe7c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_ID='speciesKey'\n",
    "grid_size_km=20\n",
    "cle_geo=\"codeMaille\"+str(grid_size_km)+'Km'\n",
    "country_name=\"France\"\n",
    "\n",
    "path=r\"C:\\Users\\anormand\\Documents\\Projet Python\\Biodiv\\Data\\GBIF_France\"\n",
    "df=pd.DataFrame()\n",
    "for i in range(1,21):\n",
    "    print(str(i))\n",
    "    df_temp=pd.read_csv(path+\"\\data_GBIF_France_\"+cle_geo+\"_periodes_\"+str(i)+\".csv\")\n",
    "    df=pd.concat([df,df_temp],ignore_index=True)\n",
    "                            \n",
    "\n",
    "df_maille_espece = df.groupby([cle_geo, cle_ID,'periode'], observed=True)['nombreObs'].sum().reset_index(name='nombreObs')\n",
    "\n",
    "df_dico=generer_dictionnaire_taxonomie(df,cle_ID)\n",
    "\n",
    "df_maille_espece=pd.merge(df_maille_espece,df_dico,on=cle_ID)\n",
    "\n",
    "df_maille_espece.to_csv(\"C:/Users/anormand/Documents/Projet Python/Biodiv/Data/GBIF_\"+country_name.replace(' ', '_')\n",
    "                  +\"/data_GBIF_\"+country_name.replace(' ', '_')+'_'+cle_geo+\"_periodes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955f6ac-9aa8-4dde-90a8-212e9e94ae4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
